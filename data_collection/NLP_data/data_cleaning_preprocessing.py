#!/usr/bin/env python3
"""
üßπ –ü—Ä–∞–∫—Ç–∏—á–Ω–∞ Data Cleaning —Å–∫—Ä–∏–ø—Ç–∞ –∑–∞ Event Data
================================================

–°–ø–µ—Ü–∏—ò–∞–ª–Ω–æ –¥–∏–∑–∞—ò–Ω–∏—Ä–∞–Ω–∞ –∑–∞:
- AlleventsEvents + CineplexEvents –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—ò–∞
- Schema —É–Ω–∏—Ñ–∏–∫–∞—Ü–∏—ò–∞
- –ú–∞–∫–µ–¥–æ–Ω—Å–∫–∏/–ê–Ω–≥–ª–∏—Å–∫–∏ —Ç–µ–∫—Å—Ç —Å—Ç–∞–Ω–¥–∞—Ä–¥–∏–∑–∞—Ü–∏—ò–∞
- Missing values –∏–Ω—Ç–µ–ª–∏–≥–µ–Ω—Ç–Ω–æ –ø–æ–ø–æ–ª–Ω—É–≤–∞—ö–µ
- GNN-ready dataset –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞
"""

import pandas as pd
import numpy as np
import re
from pathlib import Path
from datetime import datetime
import warnings

warnings.filterwarnings('ignore')


class EventDataCleaner:
    """–ü—Ä–∞–∫—Ç–∏—á–µ–Ω cleaner –∑–∞ event –ø–æ–¥–∞—Ç–æ—Ü–∏"""

    def __init__(self, data_dir="../processed_data", output_dir="cleaned_data"):
        self.data_dir = Path(data_dir)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        self.stats = {
            'original_rows': 0,
            'final_rows': 0,
            'duplicates_removed': 0,
            'values_filled': 0,
            'standardizations': 0
        }

    def load_all_csvs(self):
        """–í—á–∏—Ç–∞—ò –≥–∏ —Å–∏—Ç–µ CSV —Ñ–∞—ò–ª–æ–≤–∏"""
        dataframes = {}

        csv_files = list(self.data_dir.glob("*.csv"))
        print(f"üìÅ –ü—Ä–æ–Ω–∞—ò–¥–µ–Ω–∏ {len(csv_files)} CSV —Ñ–∞—ò–ª–æ–≤–∏")

        # –û—á–µ–∫—É–≤–∞–Ω–∏ –∏–∑–≤–æ—Ä–∏
        expected_sources = ['allevents', 'cineplexx', 'filharmonija', 'it_events', 'karti', 'mktickets']
        found_sources = []

        for file_path in csv_files:
            try:
                df = pd.read_csv(file_path, encoding='utf-8-sig')
                source_name = file_path.stem
                dataframes[source_name] = df

                # –î–µ—Ç–µ–∫—Ç–∏—Ä–∞—ò –∏–∑–≤–æ—Ä
                detected_source = 'unknown'
                for expected in expected_sources:
                    if expected in source_name.lower():
                        detected_source = expected
                        found_sources.append(expected)
                        break

                print(f"   ‚úÖ {file_path.name}: {len(df)} rows [{detected_source}]")
                self.stats['original_rows'] += len(df)
            except Exception as e:
                print(f"   ‚ùå {file_path.name}: {e}")

        # –†–∞–ø–æ—Ä—Ç–∏—Ä–∞—ò –∑–∞ –Ω–µ–¥–æ—Å—Ç–∞—Å—É–≤–∞—á–∫–∏ –∏–∑–≤–æ—Ä–∏
        missing_sources = set(expected_sources) - set(found_sources)
        if missing_sources:
            print(f"   ‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Å—É–≤–∞–∞—Ç: {', '.join(missing_sources)}")

        return dataframes

    def unify_schema(self, dataframes):
        """–£–Ω–∏—Ñ–∏—Ü–∏—Ä–∞—ò schema –º–µ—ì—É —Ä–∞–∑–ª–∏—á–Ω–∏ –∏–∑–≤–æ—Ä–∏"""
        print("\nüîó –£–Ω–∏—Ñ–∏–∫—É–≤–∞—ö–µ –Ω–∞ schema...")

        unified_dfs = []

        for source_name, df in dataframes.items():
            print(f"   üìä –û–±—Ä–∞–±–æ—Ç—É–≤–∞–º: {source_name}")

            # –ö—Ä–µ–∏—Ä–∞—ò —Å—Ç–∞–Ω–¥–∞—Ä–¥–Ω–∞ schema
            unified_df = pd.DataFrame()

            # –ó–∞–¥–æ–ª–∂–∏—Ç–µ–ª–Ω–∏ –∫–æ–ª–æ–Ω–∏
            unified_df['event_id'] = df.get('event_id', range(len(df)))
            unified_df['title'] = df.get('title', '–ë–µ–∑ –Ω–∞—Å–ª–æ–≤')
            unified_df['description'] = self._extract_description(df)
            unified_df['date_start'] = self._extract_date(df)
            unified_df['time_start'] = self._extract_time(df)
            unified_df['location'] = self._extract_location(df)
            unified_df['category'] = self._extract_category(df)
            unified_df['organizer'] = self._extract_organizer(df, source_name)
            unified_df['price_text'] = self._extract_price(df)
            unified_df['is_free'] = self._extract_is_free(df)
            unified_df['url'] = df.get('url', df.get('ticket_url', ''))
            unified_df['source'] = source_name
            unified_df['scraped_at'] = datetime.now().strftime('%Y-%m-%d')

            unified_dfs.append(unified_df)
            print(f"      ‚úÖ –£–Ω–∏—Ñ–∏—Ü–∏—Ä–∞–Ω –≤–æ {len(unified_df.columns)} –∫–æ–ª–æ–Ω–∏")

        # –°–ø–æ–∏ –≥–∏ —Å–∏—Ç–µ
        combined_df = pd.concat(unified_dfs, ignore_index=True)
        print(f"   üéØ –ö–æ–º–±–∏–Ω–∏—Ä–∞–Ω dataset: {len(combined_df)} rows")

        return combined_df

    def _extract_description(self, df):
        """–ò–∑–≤–ª–µ—á–∏ –æ–ø–∏—Å –æ–¥ —Ä–∞–∑–ª–∏—á–Ω–∏ –∫–æ–ª–æ–Ω–∏"""
        # –ü—Ä–æ–±–∞—ò —Ä–∞–∑–ª–∏—á–Ω–∏ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ —Å–ø–æ—Ä–µ–¥ –∏–∑–≤–æ—Ä–æ—Ç
        if 'description' in df.columns:
            return df['description'].fillna('')
        elif 'cast' in df.columns and 'genre' in df.columns:
            # –ó–∞ —Ñ–∏–ª–º–æ–≤–∏ (Cineplexx) - –∫–æ–º–±–∏–Ω–∏—Ä–∞—ò genre + cast + director
            descriptions = []
            for _, row in df.iterrows():
                desc_parts = []
                if pd.notna(row.get('genre', '')):
                    desc_parts.append(f"–ñ–∞–Ω—Ä: {row['genre']}")
                if pd.notna(row.get('director', '')):
                    desc_parts.append(f"–†–µ–∂–∏—Å–µ—Ä: {row['director']}")
                if pd.notna(row.get('cast', '')):
                    cast_clean = str(row['cast'])[:200]  # –õ–∏–º–∏—Ç–∏—Ä–∞—ò –¥–æ–ª–∂–∏–Ω–∞
                    desc_parts.append(f"–ì–ª—É–º—Ü–∏: {cast_clean}")
                descriptions.append('. '.join(desc_parts))
            return pd.Series(descriptions)
        elif 'categories' in df.columns:
            # –ó–∞ –Ω–∞—Å—Ç–∞–Ω–∏ —Å–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
            return df['categories'].astype(str).apply(
                lambda x: f"–ö–∞—Ç–µ–≥–æ—Ä–∏—ò–∞: {x}" if x and x != 'nan' else ''
            )
        elif 'organizer' in df.columns:
            # –ê–∫–æ –∏–º–∞ —Å–∞–º–æ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ç–æ—Ä, –∫–æ—Ä–∏—Å—Ç–∏ –≥–æ —Ç–æ—ò
            return df['organizer'].astype(str).apply(
                lambda x: f"–û—Ä–≥–∞–Ω–∏–∑–∞—Ç–æ—Ä: {x}" if x and x != 'nan' else ''
            )
        else:
            # Fallback - –ø—Ä–∞–∑–µ–Ω –æ–ø–∏—Å
            return pd.Series([''] * len(df))

    def _extract_date(self, df):
        """–°—Ç–∞–Ω–¥–∞—Ä–¥–∏–∑–∏—Ä–∞—ò –¥–∞—Ç—É–º–∏"""
        date_col = df.get('date_start', '')

        def parse_date(date_str):
            if pd.isna(date_str) or date_str == '':
                return ''

            date_str = str(date_str).strip()

            # –ú–∞–ø–∏—Ä–∞—ö–∞ –∑–∞ –º–µ—Å–µ—Ü–∏
            months_mk = {
                '—ò–∞–Ω—É–∞—Ä–∏': '01', '—Ñ–µ–≤—Ä—É–∞—Ä–∏': '02', '–º–∞—Ä—Ç': '03', '–∞–ø—Ä–∏–ª': '04',
                '–º–∞—ò': '05', '—ò—É–Ω–∏': '06', '—ò—É–ª–∏': '07', '–∞–≤–≥—É—Å—Ç': '08',
                '—Å–µ–ø—Ç–µ–º–≤—Ä–∏': '09', '–æ–∫—Ç–æ–º–≤—Ä–∏': '10', '–Ω–æ–µ–º–≤—Ä–∏': '11', '–¥–µ–∫–µ–º–≤—Ä–∏': '12'
            }

            months_en = {
                'january': '01', 'february': '02', 'march': '03', 'april': '04',
                'may': '05', 'june': '06', 'july': '07', 'august': '08',
                'september': '09', 'october': '10', 'november': '11', 'december': '12'
            }

            # Pattern: "11 August" -> "2025-08-11"
            match = re.search(r'(\d+)\s+([a-zA-Z–∞-—à–ê-–®]+)', date_str)
            if match:
                day, month_name = match.groups()
                month_name = month_name.lower()

                month_num = months_en.get(month_name) or months_mk.get(month_name)
                if month_num:
                    return f"2025-{month_num}-{day.zfill(2)}"

            # Pattern: "2025-08-11" (–≤–µ—ú–µ —Å—Ç–∞–Ω–¥–∞—Ä–¥–∏–∑–∏—Ä–∞–Ω)
            if re.match(r'\d{4}-\d{2}-\d{2}', date_str):
                return date_str

            return ''

        return date_col.apply(parse_date)

    def _extract_time(self, df):
        """–°—Ç–∞–Ω–¥–∞—Ä–¥–∏–∑–∏—Ä–∞—ò –≤—Ä–µ–º–µ"""
        time_col = df.get('time_start', df.get('duration', ''))

        def parse_time(time_str):
            if pd.isna(time_str) or time_str == '':
                return ''

            time_str = str(time_str).strip()

            # Pattern: "19:00" –∏–ª–∏ "7:00 pm"
            match = re.search(r'(\d{1,2}):(\d{2})', time_str)
            if match:
                hour, minute = match.groups()
                hour = int(hour)

                # –ü—Ä–æ–≤–µ—Ä–∏ –∑–∞ PM
                if 'pm' in time_str.lower() and hour < 12:
                    hour += 12
                elif 'am' in time_str.lower() and hour == 12:
                    hour = 0

                if 0 <= hour <= 23:
                    return f"{hour:02d}:{minute}"

            # Pattern: "20 —á–∞—Å–æ—Ç"
            match = re.search(r'(\d{1,2})\s*—á–∞—Å', time_str)
            if match:
                hour = int(match.group(1))
                if 0 <= hour <= 23:
                    return f"{hour:02d}:00"

            return ''

        return time_col.apply(parse_time)

    def _extract_location(self, df):
        """–°—Ç–∞–Ω–¥–∞—Ä–¥–∏–∑–∏—Ä–∞—ò –ª–æ–∫–∞—Ü–∏–∏"""
        location_col = df.get('location', '')

        # –†–µ—á–Ω–∏–∫ –∑–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—ò–∞ –Ω–∞ –ª–æ–∫–∞—Ü–∏–∏ - –ø—Ä–æ—à–∏—Ä–µ–Ω–æ
        location_mapping = {
            'cineplexx - skopje': 'Cineplexx - –°–∫–æ–ø—ò–µ',
            'cineplexx': 'Cineplexx - –°–∫–æ–ø—ò–µ',
            '—Å–∫–æ–ø—ò–µ': '–°–∫–æ–ø—ò–µ',
            'skopje': '–°–∫–æ–ø—ò–µ',
            'stadion na arm': '–°—Ç–∞–¥–∏–æ–Ω –Ω–∞ –ê–†–ú',
            '—Å—Ç–∞–¥–∏–æ–Ω –Ω–∞ –∞—Ä–º': '–°—Ç–∞–¥–∏–æ–Ω –Ω–∞ –ê–†–ú',
            'arm stadium': '–°—Ç–∞–¥–∏–æ–Ω –Ω–∞ –ê–†–ú',
            'mladinski kulturen centar': '–ú–ª–∞–¥–∏–Ω—Å–∫–∏ –ö—É–ª—Ç—É—Ä–µ–Ω –¶–µ–Ω—Ç–∞—Ä',
            '–º–ª–∞–¥–∏–Ω—Å–∫–∏ –∫—É–ª—Ç—É—Ä–µ–Ω —Ü–µ–Ω—Ç–∞—Ä': '–ú–ª–∞–¥–∏–Ω—Å–∫–∏ –ö—É–ª—Ç—É—Ä–µ–Ω –¶–µ–Ω—Ç–∞—Ä',
            'mkc': '–ú–ª–∞–¥–∏–Ω—Å–∫–∏ –ö—É–ª—Ç—É—Ä–µ–Ω –¶–µ–Ω—Ç–∞—Ä',
            '–º–∫—Ü': '–ú–ª–∞–¥–∏–Ω—Å–∫–∏ –ö—É–ª—Ç—É—Ä–µ–Ω –¶–µ–Ω—Ç–∞—Ä',
            'public room': 'Public Room',
            '—Ñ–∏–ª—Ö–∞—Ä–º–æ–Ω–∏—ò–∞': '–ú–∞–∫–µ–¥–æ–Ω—Å–∫–∞ –§–∏–ª—Ö–∞—Ä–º–æ–Ω–∏—ò–∞',
            'macedonian philharmonic': '–ú–∞–∫–µ–¥–æ–Ω—Å–∫–∞ –§–∏–ª—Ö–∞—Ä–º–æ–Ω–∏—ò–∞',
            '—Ñ–∏–ª–∞—Ä–º–æ–Ω–∏—ò–∞': '–ú–∞–∫–µ–¥–æ–Ω—Å–∫–∞ –§–∏–ª—Ö–∞—Ä–º–æ–Ω–∏—ò–∞',
            '–∫–∞–º–µ—Ä–Ω–∞ —Å–∞–ª–∞': '–ö–∞–º–µ—Ä–Ω–∞ —Å–∞–ª–∞ - –§–∏–ª—Ö–∞—Ä–º–æ–Ω–∏—ò–∞',
            '–º–Ω—Ç': '–ú–∞–∫–µ–¥–æ–Ω—Å–∫–∏ –ù–∞—Ä–æ–¥–µ–Ω –¢–µ–∞—Ç–∞—Ä',
            '11 –º–∞—Ä—Ç': '–ú–∞–∫–µ–¥–æ–Ω—Å–∫–∏ –ù–∞—Ä–æ–¥–µ–Ω –¢–µ–∞—Ç–∞—Ä',
            '–º–∞–∫–µ–¥–æ–Ω—Å–∫–∏ –Ω–∞—Ä–æ–¥–µ–Ω —Ç–µ–∞—Ç–∞—Ä': '–ú–∞–∫–µ–¥–æ–Ω—Å–∫–∏ –ù–∞—Ä–æ–¥–µ–Ω –¢–µ–∞—Ç–∞—Ä',
            'la ka√±a': 'La Ka√±a',
            '—Å—Ç–∞–Ω–∏—Ü–∞ 26': '–°—Ç–∞–Ω–∏—Ü–∞ 26',
            'stanica 26': '–°—Ç–∞–Ω–∏—Ü–∞ 26',
            '–∫—É—Ä—à—É–º–ª–∏ –∞–Ω': '–ö—É—Ä—à—É–º–ª–∏ –ê–Ω',
            '–≥—Ä–∞–¥—Å–∫–∏ –ø–∞—Ä–∫': '–ì—Ä–∞–¥—Å–∫–∏ –ü–∞—Ä–∫ - –°–∫–æ–ø—ò–µ',
            'city park': '–ì—Ä–∞–¥—Å–∫–∏ –ü–∞—Ä–∫ - –°–∫–æ–ø—ò–µ',
            '–ø–∞—Ä–∫': '–ì—Ä–∞–¥—Å–∫–∏ –ü–∞—Ä–∫ - –°–∫–æ–ø—ò–µ',
            'boris trajkovski': '–°–ø–æ—Ä—Ç—Å–∫–∏ –¶–µ–Ω—Ç–∞—Ä –ë–æ—Ä–∏—Å –¢—Ä–∞—ò–∫–æ–≤—Å–∫–∏',
            '—Å–ø–æ—Ä—Ç—Å–∫–∏ —Ü–µ–Ω—Ç–∞—Ä': '–°–ø–æ—Ä—Ç—Å–∫–∏ –¶–µ–Ω—Ç–∞—Ä –ë–æ—Ä–∏—Å –¢—Ä–∞—ò–∫–æ–≤—Å–∫–∏',
            'holiday inn': 'Holiday Inn - –°–∫–æ–ø—ò–µ',
            'marriott': 'Skopje Marriott Hotel',
            'diamond mall': 'Diamond Mall - –°–∫–æ–ø—ò–µ',
            '–æ–Ω–ª–∞—ò–Ω': '–û–Ω–ª–∞—ò–Ω',
            'online': '–û–Ω–ª–∞—ò–Ω',
            'virtual': '–û–Ω–ª–∞—ò–Ω'
        }

        def clean_location(location):
            if pd.isna(location) or location == '':
                return '–°–∫–æ–ø—ò–µ'  # Default

            location = str(location).strip()

            # –û—Ç—Å—Ç—Ä–∞–Ω–∏ —à—É–º
            location = re.sub(r'[^\w\s\-–ê–∞–ë–±–í–≤–ì–≥–î–¥–É—ì–ï–µ–ñ–∂–ó–∑–°—Å–ò–∏–à—ò–ö–∫–õ–ª–â—ô–ú–º–ù–Ω–ä—ö–û–æ–ü–ø–†—Ä–°—Å–¢—Ç–å—ú–£—É–§—Ñ–•—Ö–¶—Ü–ß—á–è—ü–®—à]', '', location)
            location = re.sub(r'\s+', ' ', location).strip()

            # –ù–æ—Ä–º–∞–ª–∏–∑–∏—Ä–∞—ò
            location_lower = location.lower()
            for key, normalized in location_mapping.items():
                if key in location_lower:
                    return normalized

            # –ê–∫–æ –µ –∫—Ä–∞—Ç–∫–æ, –¥–æ–¥–∞—ò "–°–∫–æ–ø—ò–µ"
            if len(location) < 3:
                return '–°–∫–æ–ø—ò–µ'

            return location.title()

        return location_col.apply(clean_location)

    def _extract_category(self, df):
        """–°—Ç–∞–Ω–¥–∞—Ä–¥–∏–∑–∏—Ä–∞—ò –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"""
        # –ü—Ä–æ–±–∞—ò —Ä–∞–∑–ª–∏—á–Ω–∏ –∫–æ–ª–æ–Ω–∏
        category_col = df.get('categories', df.get('category', df.get('genre', '')))

        def normalize_category(cat):
            if pd.isna(cat) or cat == '':
                return '–û—Å—Ç–∞–Ω–∞—Ç–æ'

            cat_str = str(cat).lower()

            # –û—Ç—Å—Ç—Ä–∞–Ω–∏ –ª–∏—Å—Ç–∏ —Ñ–æ—Ä–º–∞—Ç ['Music'] -> Music
            if cat_str.startswith('[') and cat_str.endswith(']'):
                cat_str = cat_str.strip('[]').strip("'\"")

            # –ö–∞—Ç–µ–≥–æ—Ä–∏—ò–∞ –º–∞–ø–∏—Ä–∞—ö–∞ - –ø—Ä–æ—à–∏—Ä–µ–Ω–æ –∑–∞ —Å–∏—Ç–µ –∏–∑–≤–æ—Ä–∏
            category_mapping = {
                'music': '–ú—É–∑–∏–∫–∞',
                '–º—É–∑–∏–∫–∞': '–ú—É–∑–∏–∫–∞',
                '–∫–æ–Ω—Ü–µ—Ä—Ç': '–ú—É–∑–∏–∫–∞',
                'concert': '–ú—É–∑–∏–∫–∞',
                'classical': '–ö–ª–∞—Å–∏—á–Ω–∞ –º—É–∑–∏–∫–∞',
                '–∫–ª–∞—Å–∏—á–Ω–∞': '–ö–ª–∞—Å–∏—á–Ω–∞ –º—É–∑–∏–∫–∞',
                'opera': '–û–ø–µ—Ä–∞',
                '–æ–ø–µ—Ä–∞': '–û–ø–µ—Ä–∞',
                '–±–∞–ª–µ—Ç': '–ë–∞–ª–µ—Ç',
                'ballet': '–ë–∞–ª–µ—Ç',

                'movie': '–§–∏–ª–º',
                '—Ñ–∏–ª–º': '–§–∏–ª–º',
                'cinema': '–§–∏–ª–º',
                '–∫–∏–Ω–æ': '–§–∏–ª–º',
                '—Ö–æ—Ä–æ—Ä': '–§–∏–ª–º',
                '–∫–æ–º–µ–¥–∏—ò–∞': '–§–∏–ª–º',
                '–¥—Ä–∞–º–∞': '–§–∏–ª–º',
                '–∞–∫—Ü–∏—ò–∞': '–§–∏–ª–º',

                'theater': '–¢–µ–∞—Ç–∞—Ä',
                '—Ç–µ–∞—Ç–∞—Ä': '–¢–µ–∞—Ç–∞—Ä',
                'theatre': '–¢–µ–∞—Ç–∞—Ä',
                'pret—Å—Çava': '–¢–µ–∞—Ç–∞—Ä',
                '–ø—Ä–µ—Ç—Å—Ç–∞–≤–∞': '–¢–µ–∞—Ç–∞—Ä',

                'sport': '–°–ø–æ—Ä—Ç',
                '—Å–ø–æ—Ä—Ç': '–°–ø–æ—Ä—Ç',
                'sports': '–°–ø–æ—Ä—Ç',
                '—Ñ—É–¥–±–∞–ª': '–°–ø–æ—Ä—Ç',
                'football': '–°–ø–æ—Ä—Ç',
                '–±–∞—Å–∫–µ—Ç': '–°–ø–æ—Ä—Ç',

                'business': '–î–µ–ª–æ–≤–Ω–æ',
                'conference': '–ö–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏—ò–∞',
                '–∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏—ò–∞': '–ö–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏—ò–∞',
                'seminar': '–°–µ–º–∏–Ω–∞—Ä',
                '—Å–µ–º–∏–Ω–∞—Ä': '–°–µ–º–∏–Ω–∞—Ä',
                'workshop': '–†–∞–±–æ—Ç–∏–ª–Ω–∏—Ü–∞',
                '—Ä–∞–±–æ—Ç–∏–ª–Ω–∏—Ü–∞': '–†–∞–±–æ—Ç–∏–ª–Ω–∏—Ü–∞',

                'it': 'IT/–¢–µ—Ö–Ω–æ–ª–æ–≥–∏—ò–∞',
                'technology': 'IT/–¢–µ—Ö–Ω–æ–ª–æ–≥–∏—ò–∞',
                'tech': 'IT/–¢–µ—Ö–Ω–æ–ª–æ–≥–∏—ò–∞',
                '—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—ò–∞': 'IT/–¢–µ—Ö–Ω–æ–ª–æ–≥–∏—ò–∞',
                'it event': 'IT/–¢–µ—Ö–Ω–æ–ª–æ–≥–∏—ò–∞',

                'food': '–•—Ä–∞–Ω–∞',
                '—Ö—Ä–∞–Ω–∞': '–•—Ä–∞–Ω–∞',
                '–≥–∞—Å—Ç—Ä–æ–Ω–æ–º–∏—ò–∞': '–•—Ä–∞–Ω–∞',
                '—Ñ–µ—Å—Ç–∏–≤–∞–ª –Ω–∞ —Ö—Ä–∞–Ω–∞': '–•—Ä–∞–Ω–∞',

                'art': '–£–º–µ—Ç–Ω–æ—Å—Ç',
                '—É–º–µ—Ç–Ω–æ—Å—Ç': '–£–º–µ—Ç–Ω–æ—Å—Ç',
                '–∏–∑–ª–æ–∂–±–∞': '–£–º–µ—Ç–Ω–æ—Å—Ç',
                'exhibition': '–£–º–µ—Ç–Ω–æ—Å—Ç',
                '–≥–∞–ª–µ—Ä–∏—ò–∞': '–£–º–µ—Ç–Ω–æ—Å—Ç',

                'education': '–ï–¥—É–∫–∞—Ü–∏—ò–∞',
                '–µ–¥—É–∫–∞—Ü–∏—ò–∞': '–ï–¥—É–∫–∞—Ü–∏—ò–∞',
                '–æ–±—É–∫–∞': '–ï–¥—É–∫–∞—Ü–∏—ò–∞',
                '–∫—É—Ä—Å': '–ï–¥—É–∫–∞—Ü–∏—ò–∞',
                'course': '–ï–¥—É–∫–∞—Ü–∏—ò–∞',

                'kids': '–ó–∞ –¥–µ—Ü–∞',
                '–¥–µ—Ü–∞': '–ó–∞ –¥–µ—Ü–∞',
                '–¥–µ—Ç—Å–∫–∏': '–ó–∞ –¥–µ—Ü–∞',
                'children': '–ó–∞ –¥–µ—Ü–∞',
                'family': '–°–µ–º–µ—ò–Ω–æ',
                '—Å–µ–º–µ—ò–Ω–æ': '–°–µ–º–µ—ò–Ω–æ',

                'event': '–ù–∞—Å—Ç–∞–Ω',
                '–Ω–∞—Å—Ç–∞–Ω': '–ù–∞—Å—Ç–∞–Ω'
            }

            for key, normalized in category_mapping.items():
                if key in cat_str:
                    return normalized

            return '–û—Å—Ç–∞–Ω–∞—Ç–æ'

        return category_col.apply(normalize_category)

    def _extract_organizer(self, df, source_name):
        """–ò–∑–≤–ª–µ—á–∏ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ç–æ—Ä"""
        organizer_col = df.get('organizer', '')

        def get_organizer(org):
            if pd.isna(org) or org == '':
                # Default –ø–æ –∏–∑–≤–æ—Ä - –ø—Ä–æ—à–∏—Ä–µ–Ω–æ –∑–∞ —Å–∏—Ç–µ 6 –∏–∑–≤–æ—Ä–∏
                source_defaults = {
                    'cineplexx': 'Cineplexx',
                    'allevents': 'AllEvents Contributors',
                    'filharmonija': '–ú–∞–∫–µ–¥–æ–Ω—Å–∫–∞ –§–∏–ª—Ö–∞—Ä–º–æ–Ω–∏—ò–∞',
                    'it_events': 'IT.mk',
                    'karti': 'Karti.mk',
                    'mktickets': 'MKTickets.mk'
                }

                for key, default_org in source_defaults.items():
                    if key in source_name.lower():
                        return default_org

                return '–ù–µ–ø–æ–∑–Ω–∞—Ç'
            return str(org).strip()

        return organizer_col.apply(get_organizer)

    def _extract_price(self, df):
        """–°—Ç–∞–Ω–¥–∞—Ä–¥–∏–∑–∏—Ä–∞—ò —Ü–µ–Ω–∏"""
        price_col = df.get('ticket_price_text', df.get('price_text', ''))

        def normalize_price(price):
            if pd.isna(price) or price == '':
                return '–ë–µ—Å–ø–ª–∞—Ç–Ω–æ'  # Default: –±–µ—Å–ø–ª–∞—Ç–Ω–æ –Ω–∞–º–µ—Å—Ç–æ "—Å–µ —É—Ç–æ—á—É–≤–∞"

            price_str = str(price).strip()

            # –°—Ç–∞–Ω–¥–∞—Ä–¥–∏–∑–∏—Ä–∞—ò
            if any(word in price_str.lower() for word in ['–±–µ—Å–ø–ª–∞—Ç–Ω–æ', 'free', '–±–µ–∑']):
                return '–ë–µ—Å–ø–ª–∞—Ç–Ω–æ'

            # –ù–æ—Ä–º–∞–ª–∏–∑–∏—Ä–∞—ò "250 –¥–µ–Ω." -> "250 MKD"
            price_str = re.sub(r'–¥–µ–Ω\.?', 'MKD', price_str)
            price_str = re.sub(r'–¥–µ–Ω–∞—Ä[–∏]?', 'MKD', price_str)
            price_str = re.sub(r'–µ–≤—Ä[–∞]?', 'EUR', price_str)

            return price_str

        return price_col.apply(normalize_price)

    def _extract_is_free(self, df):
        """–û–ø—Ä–µ–¥–µ–ª–∏ –¥–∞–ª–∏ –µ –±–µ—Å–ø–ª–∞—Ç–Ω–æ"""
        # –ü—Ä–æ–≤–µ—Ä–∏ –æ–¥ –ø–æ–≤–µ—ú–µ –∫–æ–ª–æ–Ω–∏
        free_col = df.get('ticket_free', df.get('is_free', True))
        price_col = df.get('ticket_price_text', df.get('price_text', ''))

        def determine_is_free(row_idx):
            # –ü—Ä–≤–∏–Ω –ø—Ä–æ–≤–µ—Ä–∏ explicit free –∫–æ–ª–æ–Ω–∞
            if not pd.isna(free_col.iloc[row_idx]):
                return bool(free_col.iloc[row_idx])

            # –ü—Ä–æ–≤–µ—Ä–∏ –æ–¥ —Ü–µ–Ω–∞
            price = str(price_col.iloc[row_idx]).lower()
            if any(word in price for word in ['–±–µ—Å–ø–ª–∞—Ç–Ω–æ', 'free', '–±–µ–∑']):
                return True
            elif any(word in price for word in ['–¥–µ–Ω', 'mkd', 'eur', '‚Ç¨', '–¥–µ–Ω–∞—Ä']):
                return False

            return True  # Default –±–µ—Å–ø–ª–∞—Ç–Ω–æ (—Å–ø–æ—Ä–µ–¥ –Ω–æ–≤–∞—Ç–∞ –ª–æ–≥–∏–∫–∞)

        return [determine_is_free(i) for i in range(len(df))]

    def clean_data(self, df):
        """–ì–ª–∞–≤–Ω–∞ —Ñ—É–Ω–∫—Ü–∏—ò–∞ –∑–∞ —á–∏—Å—Ç–µ—ö–µ"""
        print("\nüßπ –ß–∏—Å—Ç–µ—ö–µ –Ω–∞ –ø–æ–¥–∞—Ç–æ—Ü–∏...")

        original_count = len(df)

        # 1. –û—Ç—Å—Ç—Ä–∞–Ω–∏ –¥—É–ø–ª–∏–∫–∞—Ç–∏
        print("   üóëÔ∏è –û—Ç—Å—Ç—Ä–∞–Ω—É–≤–∞—ö–µ –¥—É–ø–ª–∏–∫–∞—Ç–∏...")
        duplicates_mask = df.duplicated(subset=['title', 'date_start', 'location'], keep='first')
        duplicates_count = duplicates_mask.sum()

        if duplicates_count > 0:
            df = df[~duplicates_mask].reset_index(drop=True)
            self.stats['duplicates_removed'] = duplicates_count
            print(f"      ‚ùå –û—Ç—Å—Ç—Ä–∞–Ω–µ—Ç–∏ {duplicates_count} –¥—É–ø–ª–∏–∫–∞—Ç–∏")

        # 2. –ü–æ–ø–æ–ª–Ω–∏ –ø—Ä–∞–∑–Ω–∏ –≤—Ä–µ–¥–Ω–æ—Å—Ç–∏
        print("   ‚úèÔ∏è –ü–æ–ø–æ–ª–Ω—É–≤–∞—ö–µ –ø—Ä–∞–∑–Ω–∏ –≤—Ä–µ–¥–Ω–æ—Å—Ç–∏...")

        fill_count = 0

        # –ü–æ–ø–æ–ª–Ω–∏ –ø—Ä–∞–∑–Ω–∏ –Ω–∞—Å–ª–æ–≤–∏
        empty_titles = df['title'].isna() | (df['title'] == '')
        if empty_titles.any():
            df.loc[empty_titles, 'title'] = '–ù–∞—Å—Ç–∞–Ω –±–µ–∑ –Ω–∞—Å–ª–æ–≤'
            fill_count += empty_titles.sum()

        # –ü–æ–ø–æ–ª–Ω–∏ –ø—Ä–∞–∑–Ω–∏ –¥–∞—Ç—É–º–∏ —Å–æ "–ù–µ —Å–µ –∑–Ω–∞–µ —Å–µ—É—à—Ç–µ"
        empty_dates = df['date_start'] == ''
        if empty_dates.any():
            df.loc[empty_dates, 'date_start'] = '–ù–µ —Å–µ –∑–Ω–∞–µ —Å–µ—É—à—Ç–µ'
            fill_count += empty_dates.sum()

        # –ü–æ–ø–æ–ª–Ω–∏ –ø—Ä–∞–∑–Ω–∏ –≤—Ä–µ–º–∏—ö–∞ —Å–æ default –≤—Ä–µ–¥–Ω–æ—Å—Ç–∏ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—ò–∞
        empty_times = df['time_start'] == ''
        if empty_times.any():
            def default_time(category):
                mapping = {
                    '–§–∏–ª–º': '19:00',
                    '–ú—É–∑–∏–∫–∞': '20:00',
                    '–ö–ª–∞—Å–∏—á–Ω–∞ –º—É–∑–∏–∫–∞': '19:30',
                    '–û–ø–µ—Ä–∞': '19:00',
                    '–ë–∞–ª–µ—Ç': '19:00',
                    '–¢–µ–∞—Ç–∞—Ä': '19:30',
                    '–°–ø–æ—Ä—Ç': '18:00',
                    '–ö–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏—ò–∞': '10:00',
                    'IT/–¢–µ—Ö–Ω–æ–ª–æ–≥–∏—ò–∞': '18:00',
                    '–°–µ–º–∏–Ω–∞—Ä': '14:00',
                    '–†–∞–±–æ—Ç–∏–ª–Ω–∏—Ü–∞': '10:00',
                    '–ó–∞ –¥–µ—Ü–∞': '16:00',
                    '–°–µ–º–µ—ò–Ω–æ': '17:00'
                }
                return mapping.get(category, '19:00')

            df.loc[empty_times, 'time_start'] = df.loc[empty_times, 'category'].apply(default_time)
            fill_count += empty_times.sum()

        self.stats['values_filled'] = fill_count
        print(f"      ‚úÖ –ü–æ–ø–æ–ª–Ω–µ—Ç–∏ {fill_count} –ø—Ä–∞–∑–Ω–∏ –≤—Ä–µ–¥–Ω–æ—Å—Ç–∏")

        # 3. –°—Ç–∞–Ω–¥–∞—Ä–¥–∏–∑–∏—Ä–∞—ò —Ç–µ–∫—Å—Ç
        print("   üìù –°—Ç–∞–Ω–¥–∞—Ä–¥–∏–∑–∞—Ü–∏—ò–∞ –Ω–∞ —Ç–µ–∫—Å—Ç...")

        # –°—Ç–∞–Ω–¥–∞—Ä–¥–∏–∑–∏—Ä–∞—ò –Ω–∞—Å–ª–æ–≤–∏ (Title Case)
        df['title'] = df['title'].str.strip().str.title()

        # –õ–∏–º–∏—Ç–∏—Ä–∞—ò –¥–æ–ª–∂–∏–Ω–∞ –Ω–∞ –æ–ø–∏—Å
        df['description'] = df['description'].str[:500]

        self.stats['standardizations'] += len(df)

        final_count = len(df)
        self.stats['final_rows'] = final_count

        print(f"   üìä –§–∏–Ω–∞–ª–µ–Ω dataset: {final_count} rows")
        print(f"   üìà –ó–∞–¥—Ä–∂–∞–Ω–∏: {final_count / original_count * 100:.1f}% –æ–¥ –æ—Ä–∏–≥–∏–Ω–∞–ª–Ω–∏—Ç–µ –ø–æ–¥–∞—Ç–æ—Ü–∏")

        return df

    def validate_data(self, df):
        """–í–∞–ª–∏–¥–∏—Ä–∞—ò –≥–∏ –ø–æ–¥–∞—Ç–æ—Ü–∏—Ç–µ"""
        print("\n‚úÖ –í–∞–ª–∏–¥–∞—Ü–∏—ò–∞ –Ω–∞ –ø–æ–¥–∞—Ç–æ—Ü–∏...")

        issues = []

        # –ü—Ä–æ–≤–µ—Ä–∏ –∑–∞–¥–æ–ª–∂–∏—Ç–µ–ª–Ω–∏ –ø–æ–ª–∏—ö–∞
        required_fields = ['title', 'location', 'category']
        for field in required_fields:
            empty_count = (df[field].isna() | (df[field] == '')).sum()
            if empty_count > 0:
                issues.append(f"{field}: {empty_count} –ø—Ä–∞–∑–Ω–∏ –≤—Ä–µ–¥–Ω–æ—Å—Ç–∏")

        # –ü—Ä–æ–≤–µ—Ä–∏ –¥–∞—Ç—É–º–∏
        invalid_dates = df['date_start'].apply(
            lambda x: x not in ['–ù–µ —Å–µ –∑–Ω–∞–µ —Å–µ—É—à—Ç–µ', ''] and not re.match(r'\d{4}-\d{2}-\d{2}', str(x))
        ).sum()
        if invalid_dates > 0:
            issues.append(f"date_start: {invalid_dates} –Ω–µ–≤–∞–ª–∏–¥–Ω–∏ –¥–∞—Ç—É–º–∏")

        # –ü—Ä–æ–≤–µ—Ä–∏ –≤—Ä–µ–º–∏—ö–∞
        invalid_times = df['time_start'].apply(
            lambda x: x != '' and not re.match(r'\d{2}:\d{2}', str(x))
        ).sum()
        if invalid_times > 0:
            issues.append(f"time_start: {invalid_times} –Ω–µ–≤–∞–ª–∏–¥–Ω–∏ –≤—Ä–µ–º–∏—ö–∞")

        if issues:
            print("   ‚ö†Ô∏è –ü—Ä–æ–Ω–∞—ò–¥–µ–Ω–∏ –ø—Ä–æ–±–ª–µ–º–∏:")
            for issue in issues:
                print(f"      - {issue}")
        else:
            print("   ‚úÖ –°–∏—Ç–µ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –ø–æ–º–∏–Ω–∞—Ç–∏!")

        return len(issues) == 0

    def generate_summary(self, df):
        """–ì–µ–Ω–µ—Ä–∏—Ä–∞—ò —Ä–µ–∑–∏–º–µ"""
        print("\nüìä –†–ï–ó–ò–ú–ï –û–î –ß–ò–°–¢–ï–ä–ï–¢–û")
        print("=" * 40)
        print(f"–û—Ä–∏–≥–∏–Ω–∞–ª–Ω–∏ rows: {self.stats['original_rows']:,}")
        print(f"–§–∏–Ω–∞–ª–Ω–∏ rows: {self.stats['final_rows']:,}")
        print(f"–î—É–ø–ª–∏–∫–∞—Ç–∏ –æ—Ç—Å—Ç—Ä–∞–Ω–µ—Ç–∏: {self.stats['duplicates_removed']:,}")
        print(f"–í—Ä–µ–¥–Ω–æ—Å—Ç–∏ –ø–æ–ø–æ–ª–Ω–µ—Ç–∏: {self.stats['values_filled']:,}")
        print(f"–£—Å–ø–µ—à–Ω–æ—Å—Ç: {self.stats['final_rows'] / self.stats['original_rows'] * 100:.1f}%")

        print(f"\nüìà –î–ò–°–¢–†–ò–ë–£–¶–ò–à–ê –ü–û –ö–ê–¢–ï–ì–û–†–ò–ò:")
        category_counts = df['category'].value_counts()
        for category, count in category_counts.head(10).items():
            print(f"   {category}: {count}")

        print(f"\nüìç –¢–û–ü –õ–û–ö–ê–¶–ò–ò:")
        location_counts = df['location'].value_counts()
        for location, count in location_counts.head(5).items():
            print(f"   {location}: {count}")

        print(f"\nüè¢ –¢–û–ü –û–†–ì–ê–ù–ò–ó–ê–¢–û–†–ò:")
        organizer_counts = df['organizer'].value_counts()
        for organizer, count in organizer_counts.head(5).items():
            print(f"   {organizer}: {count}")

        print(f"\nüí∞ –¶–ï–ù–ò:")
        free_events = df['is_free'].sum()
        paid_events = len(df) - free_events
        print(f"   –ë–µ—Å–ø–ª–∞—Ç–Ω–∏ –Ω–∞—Å—Ç–∞–Ω–∏: {free_events}")
        print(f"   –ü–ª–∞—Ç–µ–Ω–∏ –Ω–∞—Å—Ç–∞–Ω–∏: {paid_events}")
        if free_events > paid_events:
            print(f"   üìù –ó–∞–±–µ–ª–µ—à–∫–∞: –ù–∞—Å—Ç–∞–Ω–∏ –±–µ–∑ –ø–æ–∑–Ω–∞—Ç–∞ —Ü–µ–Ω–∞ —Å–µ —Ç—Ä–µ—Ç–∏—Ä–∞–∞—Ç –∫–∞–∫–æ –±–µ—Å–ø–ª–∞—Ç–Ω–∏")

    def run_full_cleaning(self):
        """–ò–∑–≤—Ä—à–∏ —Ü–µ–ª–æ—Å–Ω–æ —á–∏—Å—Ç–µ—ö–µ"""
        print("üöÄ –°–¢–ê–†–¢–£–í–ê–ä–ï –ù–ê DATA CLEANING –ü–†–û–¶–ï–°")
        print("=" * 50)

        # 1. –í—á–∏—Ç–∞—ò –ø–æ–¥–∞—Ç–æ—Ü–∏
        dataframes = self.load_all_csvs()
        if not dataframes:
            print("‚ùå –ù–µ–º–∞ –ø–æ–¥–∞—Ç–æ—Ü–∏ –∑–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∞!")
            return None

        # 2. –£–Ω–∏—Ñ–∏—Ü–∏—Ä–∞—ò schema
        unified_df = self.unify_schema(dataframes)

        # 3. –ò—Å—á–∏—Å—Ç–∏ –ø–æ–¥–∞—Ç–æ—Ü–∏
        cleaned_df = self.clean_data(unified_df)

        # 4. –í–∞–ª–∏–¥–∏—Ä–∞—ò
        is_valid = self.validate_data(cleaned_df)

        # 5. –ó–∞—á—É–≤–∞—ò
        output_file = self.output_dir / f"events_cleaned_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        cleaned_df.to_csv(output_file, index=False, encoding='utf-8-sig')
        print(f"\nüíæ –ó–∞—á—É–≤–∞–Ω –≤–æ: {output_file}")

        # 6. –†–µ–∑–∏–º–µ
        self.generate_summary(cleaned_df)

        # 7. –°–æ–∑–¥–∞—ò GNN-ready –≤–µ—Ä–∑–∏—ò–∞
        gnn_file = self.output_dir / "events_gnn_ready.csv"

        # –î–æ–¥–∞—ò —É–Ω–∏–∫–∞—Ç–Ω–∏ ID-ja
        cleaned_df['node_id'] = range(len(cleaned_df))
        cleaned_df.to_csv(gnn_file, index=False, encoding='utf-8-sig')
        print(f"üìä GNN-ready dataset: {gnn_file}")

        print(f"\nüéâ DATA CLEANING –ó–ê–í–†–®–ï–ù –£–°–ü–ï–®–ù–û!")
        print(f"üìÅ –ü—Ä–æ–≤–µ—Ä–∏ —ò–∞ –ø–∞–ø–∫–∞—Ç–∞ 'cleaned_data' –∑–∞ —Ä–µ–∑—É–ª—Ç–∞—Ç–∏")

        return cleaned_df


def main():
    """–ì–ª–∞–≤–Ω–∞ —Ñ—É–Ω–∫—Ü–∏—ò–∞"""
    try:
        cleaner = EventDataCleaner()
        result = cleaner.run_full_cleaning()

        if result is not None:
            print("\n‚úÖ –ü—Ä–æ—Ü–µ—Å–æ—Ç –∑–∞–≤—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
            print("üìã –ù–∞—Ä–µ–¥–Ω–∏ —á–µ–∫–æ—Ä–∏:")
            print("   1. –ü—Ä–æ–≤–µ—Ä–∏ –≥–æ cleaned dataset")
            print("   2. –ó–∞–ø–æ—á–Ω–∏ —Å–æ graph construction")
            print("   3. –†–∞–∑–≤–∏–≤–∞—ò GNN –º–æ–¥–µ–ª–∏")

    except Exception as e:
        print(f"‚ùå –ì—Ä–µ—à–∫–∞: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()