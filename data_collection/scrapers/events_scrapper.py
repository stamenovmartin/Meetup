# -*- coding: utf-8 -*-
"""Events_scrapper.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PR_MD-zh6zEpZRjPCZYEnR_sOp7RChEc

#Extracting data from filharmonija.mk and MkTickets.com
---
"""

!pip install -q requests beautifulsoup4 pandas dateparser tqdm

import urllib.robotparser
import requests
from urllib.parse import urljoin

BASE = "https://mktickets.mk/"

rp = urllib.robotparser.RobotFileParser()
rp.set_url(urljoin(BASE, "robots.txt"))
try:
    rp.read()
    allowed = rp.can_fetch("*", BASE)
    print("robots.txt fetched. Can fetch root?:", allowed)
except Exception as e:
    print("Couldn't fetch robots.txt automatically (will print exception).")
    print(e)
    print("Please open: https://mktickets.mk/robots.txt in your browser and check rules manually.")
print("\nPlease review Terms / policies before scraping:")
print(" - Terms: https://mktickets.mk/en/terms-of-services/")  # English terms page
print(" - Правила и политики: https://mktickets.mk/праваила-и-политики-за-користење/ (Macedonian)")

import time
import re
import requests
from bs4 import BeautifulSoup
import pandas as pd
from tqdm import tqdm
import dateparser

BASE_URL = "https://mktickets.mk"
HEADERS = {
    "User-Agent": "ResearchBot/0.1 (+https://your-institution-or-email.example)"
}
SLEEP = 1.5
MAX_CATEGORY_PAGES = 75

session = requests.Session()
session.headers.update(HEADERS)

def get_soup(url, timeout=15):
    r = session.get(url, timeout=timeout)
    r.raise_for_status()
    return BeautifulSoup(r.text, "html.parser")

def normalize_url(href):
    if not href:
        return None
    if href.startswith("http"):
        return href.split('?')[0].split('#')[0]
    if href.startswith("/"):
        return BASE_URL + href.split('?')[0]
    return urljoin(BASE_URL, href.split('?')[0])

def extract_event_links_from_page(soup):
    """Return unique event page URLs found on a listing page (best-effort)."""
    links = set()
    for a in soup.find_all("a", href=True):
        href = a["href"]
        href_lower = href.lower()


        if "/event/" in href_lower or "/event-en/" in href_lower:
            event_pos = href_lower.find("/event/")
            if event_pos == -1:
                event_pos = href_lower.find("/event-en/")
            page_pos = href_lower.find("/page/", event_pos)
            if page_pos != -1:
                continue
            links.add(normalize_url(href))
    return links

seed_urls = [
    BASE_URL + "/en/all-events/",
    BASE_URL + "/all-events/",
    BASE_URL + "/nastani/",
    BASE_URL + "/mk/nastani/",
    BASE_URL + "/category/event/"  # category archive (pagination pattern)
]

found_event_urls = set()

for seed in seed_urls:
    try:
        soup = get_soup(seed)
    except Exception as e:
        continue
    found_event_urls |= extract_event_links_from_page(soup)
    time.sleep(SLEEP)

for page in range(1, MAX_CATEGORY_PAGES + 1):
    page_url = f"{BASE_URL}/category/event/page/{page}/"
    try:
        soup = get_soup(page_url)
    except requests.HTTPError as e:
        break
    links = extract_event_links_from_page(soup)
    if not links:
        break
    found_event_urls |= links
    time.sleep(SLEEP)

print("Found", len(found_event_urls), "unique event URLs (sample):")
for u in list(found_event_urls)[:10]:
    print(" -", u)

import hashlib
MAX_EVENTS = 764
CATEGORY_KEYWORDS = {
    "festival": ["festival", "фестивал", "фестивалот"],
    "concert": ["concert", "концерт", "live music", "музика","бендови "],
    "theatre": ["theatre", "театар", "драма", "play", "плеј"],
    "sports": ["sport", "спорт", "фудбал", "баскет", "спортски"],
    "exhibition": ["exhibition", "изложба", "експозиција"],
    "workshop": ["workshop", "работилница", "курс"],
    "party": ["журка", "party", "вечер", "жешка журка"],
    "standup_comedy": ["stendup", "комедија", "stand-up", "standup", "stand up", "стендап"],
    "dance": ["dance", "танц", "танцување"],
    "movie": ["movie", "филм", "кинo"],
    "kids": ["деца", "kids", "детска", "детски"],
    "ballet":["балет","балетска","балерина","ballet"]
}
def detect_categories_from_text(text):
    if not text:
        return []
    text_lower = text.lower()
    detected = set()
    for category, keywords in CATEGORY_KEYWORDS.items():
        for kw in keywords:
            if kw in text_lower:
                detected.add(category)
                break
    return list(detected)
MAX_PAGES = 5
found_event_urls = set()

for page_num in range(1, MAX_PAGES + 1):
    page_url = f"https://www.filharmonija.mk/events/page/{page_num}/"
    print(f"Scraping event list page: {page_url}")
    try:
        soup = get_soup(page_url)
    except Exception as e:
        print(f"Failed to load page {page_num}: {e}")
        break
    found_event_urls |= extract_event_links_from_page(soup)
    time.sleep(SLEEP)

print(f"Found {len(found_event_urls)} event URLs total.")
def parse_event_page(url):
    try:
        soup = get_soup(url)
    except Exception as e:
        return {"url": url, "error": str(e)}

    data = {}
    data['event_id'] = hashlib.md5(url.encode()).hexdigest()
    data['url'] = url

    # Title
    h1 = soup.find("h1")
    data["title"] = h1.get_text(strip=True) if h1 else None

    full_text = soup.get_text(separator="\n", strip=True)
    lines = full_text.splitlines()

    mk_months = ["јануари", "февруари", "март", "април", "мај", "јуни", "јули", "август", "септември", "октомври", "ноември", "декември"]
    date_start = None
    for line in lines:
        line_lower = line.lower()
        if any(month in line_lower for month in mk_months):
            date_start = line.strip()
            break
    data["date_start"] = date_start
    time_start = None
    for line in lines:
        if "часот" in line.lower():
            time_match = re.search(r"(\d{1,2}:\d{2})", line)
            if time_match:
                time_start = time_match.group(1)
                break
    data["time_start"] = time_start
    data["location"] = "Филхармонија - Скопје"
    price_match = re.search(r"Price\s*[:\-]?\s*([\w\d\s,.]+)", full_text, re.IGNORECASE)
    data["ticket_price_text"] = price_match.group(1).strip() if price_match else None
    data["ticket_free"] = False

    ticket_url = None
    for a in soup.find_all("a", href=True):
        txt = (a.get_text() or "").lower()
        if "куп" in txt or "ticket" in txt:
            ticket_url = normalize_url(a["href"])
            break
    data["ticket_url"] = ticket_url

    # Description extraction (as before)
    desc = None
    cand = soup.find(class_=re.compile(r"(entry-content|event-desc|post-content|content)", re.I))
    if cand:
        desc = cand.get_text(separator="\n", strip=True)[:3000]
    else:
        if h1:
            p = h1.find_next("p")
            if p:
                desc = p.get_text(strip=True)
    data["description"] = desc

    # Categories (same as before)
    categories = []
    if data["title"]:
        title_lower = data["title"].lower()
        if "concert" in title_lower or "концерт" in title_lower:
            categories.append("concert")
        if "festival" in title_lower or "фестивал" in title_lower:
            categories.append("festival")
        if "standup" in title_lower or "стендап" in title_lower:
            categories.append("standup")
    data["categories"] = categories if categories else None
    organizer = None
    for line in lines:
        if "organizer" in line.lower() or "организатор" in line.lower():
            organizer = line.strip()
            break
    data["organizer"] = organizer

    return data

seed_urls = [
    BASE_URL + "/en/all-events/",
    BASE_URL + "/all-events/",
    BASE_URL + "/nastani/",
    BASE_URL + "/mk/nastani/",
    BASE_URL + "/category/event/"
]

found_event_urls = set()

for seed in seed_urls:
    try:
        soup = get_soup(seed)
    except Exception:
        continue
    found_event_urls |= extract_event_links_from_page(soup)
    if len(found_event_urls) >= MAX_EVENTS:
        break
    time.sleep(SLEEP)

if len(found_event_urls) < MAX_EVENTS:
    for page in range(1, MAX_CATEGORY_PAGES + 1):
        page_url = f"{BASE_URL}/category/event/page/{page}/"
        try:
            soup = get_soup(page_url)
        except requests.HTTPError:
            break
        links = extract_event_links_from_page(soup)
        if not links:
            break
        found_event_urls |= links
        if len(found_event_urls) >= MAX_EVENTS:
            break
        time.sleep(SLEEP)

found_event_urls = list(found_event_urls)[:MAX_EVENTS]
print(f"Found {len(found_event_urls)} unique event URLs:")

for url in found_event_urls:
    print(" -", url)
results = []
for url in tqdm(found_event_urls):
    results.append(parse_event_page(url))
    time.sleep(SLEEP)

df = pd.DataFrame(results)
print(df.head(10))
csv_path = "/content/mktickets_events.csv"
df.to_csv(csv_path, index=False, encoding="utf-8-sig")
print(f"Scraped data saved to {csv_path}")

"""#From Filharmonija.mk"""

import requests
from bs4 import BeautifulSoup
import hashlib
import re
import time
import pandas as pd
from tqdm import tqdm
import urllib.parse

BASE_URL = "https://www.filharmonija.mk"
START_URL = BASE_URL + "/архива-настани/"
MAX_EVENTS = 300
SLEEP = 1.0

def normalize_url(href):
    if href.startswith("http"):
        return href
    else:
        return BASE_URL.rstrip("/") + "/" + href.lstrip("/")

def get_soup(url):
    headers = {
        "User-Agent": "Mozilla/5.0 (compatible; EventScraper/1.0)"
    }
    resp = requests.get(url, headers=headers)
    resp.raise_for_status()
    return BeautifulSoup(resp.text, "html.parser")

def extract_event_links_from_page(soup):
    links = set()

    for a in soup.find_all("a", href=True):
        href = a['href']
        # Philharmonic uses /events/ in event URLs (adjust if needed)
        if "/events/" in href or "/event/" in href:
            full_url = normalize_url(href)
            links.add(full_url)
    return links

def parse_event_page(url):
    try:
        soup = get_soup(url)
    except Exception as e:
        return {"url": url, "error": str(e)}

    data = {}
    data['event_id'] = hashlib.md5(url.encode()).hexdigest()
    data['url'] = url

    # Title extracted from URL
    path = urllib.parse.urlparse(url).path
    last_part = path.split('/')[-1] or path.split('/')[-2]
    title_from_url = urllib.parse.unquote(last_part).replace('-', ' ').strip()
    data["title"] = title_from_url

    full_text = soup.get_text(separator="\n", strip=True)
    lines = full_text.splitlines()

    mk_months = ["јануари", "февруари", "март", "април", "мај", "јуни", "јули", "август", "септември", "октомври", "ноември", "декември"]
    date_start = None
    for line in lines:
        line_lower = line.lower()
        if any(month in line_lower for month in mk_months):
            date_start = line.strip()
            break
    data["date_start"] = date_start

    time_start = None
    for line in lines:
        if "часот" in line.lower():
            time_match = re.search(r"(\d{1,2}:\d{2})", line)
            if time_match:
                time_start = time_match.group(1)
                break
    data["time_start"] = time_start

    data["location"] = "Филхармонија - Скопје"

    # Initialize ticket_url here
    ticket_url = None
    for a in soup.find_all("a", href=True):
        txt = (a.get_text() or "").lower()
        if "куп" in txt or "buy ticket" in txt or "купибилет" in txt.replace(" ", ""):
            ticket_url = normalize_url(a['href'])
            break

    ticket_price_text = None
    if ticket_url:
        try:
           ticket_soup = get_soup(ticket_url)
           ticket_text = ticket_soup.get_text(separator="\n", strip=True)
           print(f"Fetching ticket page: {ticket_url}")
           ticket_text = ticket_soup.get_text(separator="\n", strip=True)

           price_match = re.search(r"Цена.*?(\d{2,6})", ticket_text, re.IGNORECASE)
           if price_match:
                ticket_price_text = price_match.group(1).strip()
        except Exception as e:
            print(f"Could not get ticket price from {ticket_url}: {e}")

    data["ticket_url"] = ticket_url
    data["ticket_price_text"] = ticket_price_text
    data["ticket_free"] = False  # You can expand logic here if needed

    desc = None
    cand = soup.find(class_=re.compile(r"(entry-content|event-desc|post-content|content)", re.I))
    if cand:
        desc = cand.get_text(separator="\n", strip=True)[:3000]
    else:
        h1 = soup.find("h1")
        if h1:
            p = h1.find_next("p")
            if p:
                desc = p.get_text(strip=True)
    data["description"] = desc

    categories = []
    text_to_check = (data["title"] or "") + " " + (data["description"] or "")
    text_lower = text_to_check.lower()
    if "концерт" in text_lower or "concert" in text_lower:
        categories.append("concert")
    if "фестивал" in text_lower or "festival" in text_lower:
        categories.append("festival")
    if "стендап" in text_lower or "standup" in text_lower:
        categories.append("standup")
    if "журка" in text_lower or "party" in text_lower:
        categories.append("party")
    data["categories"] = categories if categories else None

    data["organizer"] = "Филхармонија"

    return data
# ==== MAIN SCRAPING LOGIC ====

# Step 1: Get event URLs from archive page(s)
# ==== MAIN SCRAPING LOGIC WITH PAGINATION ====

MAX_PAGES = 28  # max pages to scrape; adjust if you want more or less
found_event_urls = set()

for page_num in range(1, MAX_PAGES + 1):
    if page_num == 1:
        page_url = BASE_URL + "/архива-настани/"
    else:
        page_url = f"{BASE_URL}/архива-настани/page/{page_num}/"

    print(f"Scraping page {page_num}: {page_url}")
    try:
        soup = get_soup(page_url)
    except Exception as e:
        print(f"Failed to load page {page_num}: {e}")
        break

    new_links = extract_event_links_from_page(soup)
    if not new_links:
        print("No more events found, stopping.")
        break

    before_count = len(found_event_urls)
    found_event_urls |= new_links
    after_count = len(found_event_urls)
    print(f"Found {after_count - before_count} new event links on page {page_num}.")

    time.sleep(SLEEP)

found_event_urls = list(found_event_urls)  # convert to list for processing

print(f"Total unique events found: {len(found_event_urls)}")

# Optionally limit number of events to scrape
if len(found_event_urls) > MAX_EVENTS:
    found_event_urls = found_event_urls[:MAX_EVENTS]


# Step 2: Parse each event page
results = []
for url in tqdm(found_event_urls):
    event_data = parse_event_page(url)
    results.append(event_data)
    time.sleep(SLEEP)

# Step 3: Save results to DataFrame and CSV
df = pd.DataFrame(results)
print(df.head())

csv_path = "/content/filharmonija_events.csv"
df.to_csv(csv_path, index=False, encoding="utf-8-sig")
print(f"Saved scraped data to {csv_path}")

"""#Facebook scraper

"""

!apt-get update
!apt install -y chromium-chromedriver
!pip install selenium

!apt-get update
!apt-get install chromium-browser
!pip install selenium
!wget https://chromedriver.storage.googleapis.com/114.0.5735.90/chromedriver_linux64.zip
!unzip chromedriver_linux64.zip
!chmod +x chromedriver

# Facebook Events Scraper for Google Colab
# Note: This is for educational purposes. Always respect Facebook's Terms of Service.

# Install required packages
!pip install requests beautifulsoup4 selenium webdriver-manager pandas lxml facebook-sdk

import requests
from bs4 import BeautifulSoup
import pandas as pd
import json
import time
import re
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
import facebook  # Facebook SDK

# Method 1: Using Facebook Graph API (Recommended)
class FacebookGraphAPI:
    def __init__(self, access_token):
        self.graph = facebook.GraphAPI(access_token=access_token, version="3.1")

    def search_events(self, query, location=None, limit=50):
        """
        Search for public events using Facebook Graph API
        Note: As of 2024, event search is very limited in Graph API
        """
        try:
            # This endpoint has limited functionality now
            events = self.graph.request('/search', {
                'q': query,
                'type': 'event',
                'limit': limit
            })
            return events.get('data', [])
        except Exception as e:
            print(f"Graph API Error: {e}")
            return []

# Method 2: Web Scraping with Selenium (Use with caution)
class FacebookEventsScraper:
    def __init__(self, headless=True):
        self.setup_driver(headless)

    def setup_driver(self, headless=True):
        """Setup Chrome driver for Colab"""
        options = Options()
        if headless:
            options.add_argument('--headless')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-gpu')
        options.add_argument('--window-size=1920x1080')
        options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')

        # Updated for Selenium 4
        from selenium.webdriver.chrome.service import Service
        service = Service(ChromeDriverManager().install())
        self.driver = webdriver.Chrome(service=service, options=options)

    def scrape_public_events(self, location, event_type=None, max_events=20):
        """
        Scrape public events from Facebook
        This method may not work reliably due to Facebook's protections
        """
        events = []

        try:
            # Facebook events search URL
            base_url = "https://www.facebook.com/events/search/"
            params = f"?q={location}"
            if event_type:
                params += f"%20{event_type}"

            url = base_url + params

            print(f"Accessing: {url}")
            self.driver.get(url)

            # Wait for content to load
            time.sleep(5)

            # Try to find event elements
            event_elements = self.driver.find_elements(By.CSS_SELECTOR, '[data-testid="event-card"]')

            if not event_elements:
                # Try alternative selectors
                event_elements = self.driver.find_elements(By.CSS_SELECTOR, 'div[role="article"]')

            print(f"Found {len(event_elements)} event elements")

            for i, element in enumerate(event_elements[:max_events]):
                try:
                    event_data = self.extract_event_data(element)
                    if event_data:
                        events.append(event_data)
                        print(f"Scraped event {i+1}: {event_data.get('title', 'No title')}")
                except Exception as e:
                    print(f"Error extracting event {i+1}: {e}")
                    continue

        except Exception as e:
            print(f"Scraping error: {e}")

        finally:
            self.driver.quit()

        return events

    def extract_event_data(self, element):
        """Extract event information from element"""
        event_data = {}

        try:
            # Try to find title
            title_elem = element.find_element(By.CSS_SELECTOR, 'h3, [role="heading"]')
            event_data['title'] = title_elem.text if title_elem else 'No title'
        except:
            event_data['title'] = 'No title'

        try:
            # Try to find date
            date_elem = element.find_element(By.CSS_SELECTOR, '[data-testid="event-date"], time')
            event_data['date'] = date_elem.text if date_elem else 'No date'
        except:
            event_data['date'] = 'No date'

        try:
            # Try to find location
            location_elem = element.find_element(By.CSS_SELECTOR, '[data-testid="event-location"]')
            event_data['location'] = location_elem.text if location_elem else 'No location'
        except:
            event_data['location'] = 'No location'

        try:
            # Try to find link
            link_elem = element.find_element(By.CSS_SELECTOR, 'a[href*="/events/"]')
            event_data['url'] = link_elem.get_attribute('href') if link_elem else 'No URL'
        except:
            event_data['url'] = 'No URL'

        return event_data

# Method 3: Alternative APIs (Eventbrite, Meetup, etc.)
class AlternativeEventSources:
    @staticmethod
    def get_eventbrite_events(location, token=None):
        """Get events from Eventbrite API"""
        if not token:
            print("Eventbrite token required")
            return []

        url = "https://www.eventbriteapi.com/v3/events/search/"
        headers = {"Authorization": f"Bearer {token}"}
        params = {
            "location.address": location,
            "expand": "venue",
            "sort_by": "date"
        }

        try:
            response = requests.get(url, headers=headers, params=params)
            data = response.json()
            return data.get('events', [])
        except Exception as e:
            print(f"Eventbrite API error: {e}")
            return []

    @staticmethod
    def get_meetup_events(location, api_key=None):
        """Get events from Meetup API"""
        # Note: Meetup API access is limited
        # This is a placeholder for the structure
        print("Meetup API requires special access")
        return []

# Main execution function
def main():
    """Main function to run the event scraper"""

    print("Facebook Events Scraper")
    print("=======================")

    # Configuration
    LOCATION = "New York"  # Change this to your desired location
    EVENT_TYPE = "music"   # Optional: specify event type
    MAX_EVENTS = 10

    # Method 1: Try Facebook Graph API (requires token)
    print("\n1. Trying Facebook Graph API...")
    # You need to get an access token from Facebook Developer Console
    FB_ACCESS_TOKEN = None  # Replace with your token

    if FB_ACCESS_TOKEN:
        fb_api = FacebookGraphAPI(FB_ACCESS_TOKEN)
        api_events = fb_api.search_events(f"{LOCATION} {EVENT_TYPE}")
        print(f"Found {len(api_events)} events via Graph API")
    else:
        print("No Facebook access token provided")
        api_events = []

    # Method 2: Web scraping (use with caution)
    print("\n2. Trying web scraping...")
    print("Warning: This may not work due to Facebook's anti-scraping measures")

    # Uncomment the lines below to try scraping (at your own risk)
    scraper = FacebookEventsScraper(headless=True)
    scraped_events = scraper.scrape_public_events(LOCATION, EVENT_TYPE, MAX_EVENTS)
    print(f"Scraped {len(scraped_events)} events")

    scraped_events = []  # Comment this if you uncomment above

    # Method 3: Alternative sources
    print("\n3. Trying alternative sources...")
    alt_sources = AlternativeEventSources()

    # You can get Eventbrite token from their developer portal
    EVENTBRITE_TOKEN = None  # Replace with your token

    if EVENTBRITE_TOKEN:
        eventbrite_events = alt_sources.get_eventbrite_events(LOCATION, EVENTBRITE_TOKEN)
        print(f"Found {len(eventbrite_events)} events on Eventbrite")
    else:
        print("No Eventbrite token provided")
        eventbrite_events = []

    # Combine and save results
    all_events = {
        'facebook_api': api_events,
        'facebook_scraped': scraped_events,
        'eventbrite': eventbrite_events
    }

    # Save to files
    for source, events in all_events.items():
        if events:
            df = pd.DataFrame(events)
            filename = f"{source}_events_{LOCATION.replace(' ', '_')}.csv"
            df.to_csv(filename, index=False)
            print(f"Saved {len(events)} events to {filename}")

    return all_events

# Instructions for getting API tokens
def show_api_instructions():
    """Display instructions for getting API access"""
    print("""
    API Token Instructions:
    ======================

    1. Facebook Graph API:
       - Go to https://developers.facebook.com/
       - Create an app
       - Get access token from Graph API Explorer
       - Note: Event search is very limited

    2. Eventbrite API:
       - Go to https://www.eventbrite.com/platform/
       - Create an account and app
       - Get your API token
       - Free tier available

    3. Meetup API:
       - Go to https://www.meetup.com/api/
       - API access is limited and requires approval

    Legal and Ethical Notes:
    =======================
    - Always respect websites' Terms of Service
    - Don't overwhelm servers with requests
    - Consider rate limiting
    - Use official APIs when available
    - Be transparent about data collection
    """)

if __name__ == "__main__":
    # Show instructions first
    show_api_instructions()

    # Run the scraper (uncommented for testing)
    events = main()

    print("\nTo run the scraper, uncomment the line above and add your API tokens")

